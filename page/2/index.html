<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"zhuang-weiming.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.3.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>
<meta property="og:type" content="website">
<meta property="og:title" content="Zhuang&#39;s Diary">
<meta property="og:url" content="https://zhuang-weiming.github.io/page/2/index.html">
<meta property="og:site_name" content="Zhuang&#39;s Diary">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Weiming Zhuang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://zhuang-weiming.github.io/page/2/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>
<title>Zhuang's Diary</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhuang's Diary</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">言之有物，持之以恒</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Weiming Zhuang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">275</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">70</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/zhuangweiming/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;zhuangweiming&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuang-weiming.github.io/2025/03/31/Cloud+AI%E6%97%B6%E9%97%B4%E4%B8%8B%E8%B6%85%E5%A4%A7%E5%9E%8BIT%E5%9B%A2%E9%98%9F%E7%BB%A9%E6%95%88%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Weiming Zhuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuang's Diary">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/03/31/Cloud+AI%E6%97%B6%E9%97%B4%E4%B8%8B%E8%B6%85%E5%A4%A7%E5%9E%8BIT%E5%9B%A2%E9%98%9F%E7%BB%A9%E6%95%88%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB/" class="post-title-link" itemprop="url">Cloud+AI时间下超大型IT团队绩效指标体系</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-03-31 11:45:00 / Modified: 15:49:41" itemprop="dateCreated datePublished" datetime="2025-03-31T11:45:00+08:00">2025-03-31</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>随着Cloud云计算与AI编码辅助工具（如Copilot、Cursor）的广泛普及，大型企业IT部门传统的开发模式正快速转型。大量重复性工作实现自动化，工程师的工作重心逐渐向更高价值的业务交付、风险控制和创新转移。这就促使我们对超大型企业IT团队的绩效管理方法进行全新的审视与重构。<br>在新的绩效体系中，我们刻意削减传统的量化指标（如代码行数、任务数量），取而代之的是更能体现业务影响力和技术稳健性的指标。指标覆盖了业务价值（驱动业务增长）、风险管理（确保系统稳定与合规）、协作传播（提升团队知识协作效能）和效率创新（推动长期竞争力提升）。</p>
<h3 id="超大型IT团队绩效指标体系（Cloud-AI-era）"><a href="#超大型IT团队绩效指标体系（Cloud-AI-era）" class="headerlink" title="超大型IT团队绩效指标体系（Cloud+AI era）"></a>超大型IT团队绩效指标体系（Cloud+AI era）</h3><p><strong>总权重分配原则</strong>：业务价值（35%） &gt; 质量与风险（30%） &gt; 协作与知识（20%） &gt; 效率与创新（15%）  </p>
<h4 id="1-业务价值贡献（35-）"><a href="#1-业务价值贡献（35-）" class="headerlink" title="1. 业务价值贡献（35%）"></a><strong>1. 业务价值贡献（35%）</strong></h4><table>
<thead>
<tr>
<th>指标名称</th>
<th>定义与解释</th>
<th>权重</th>
<th>数据获取方式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>业务需求交付率</strong></td>
<td>年度关键业务需求（如跨境支付系统升级）按时交付的百分比</td>
<td>15%</td>
<td>Jira任务完成统计、PMO项目管理报告、代码行数</td>
</tr>
<tr>
<td><strong>客户满意度（NPS）</strong></td>
<td>内部/外部客户对IT服务的净推荐值（如API响应速度、系统易用性）</td>
<td>5%</td>
<td>定期用户满意度调查、SurveyMonkey报告</td>
</tr>
<tr>
<td><strong>云成本优化率</strong></td>
<td>通过云资源调度（如Spot实例、自动伸缩）节省的年度成本比例（例：降低15%）</td>
<td>5%</td>
<td>云平台计费报告（如AWS Cost Explorer）</td>
</tr>
<tr>
<td><strong>AI工具业务渗透率</strong></td>
<td>AI生成代码在核心业务系统（如风控模型）中的占比及有效性验证</td>
<td>5%</td>
<td>GitHub Copilot Insights、Git Commit分析</td>
</tr>
<tr>
<td><strong>交易系统延迟优化</strong></td>
<td>高频交易系统延迟降低百分比（如从5ms降至3ms）</td>
<td>5%</td>
<td>实时监控工具（如Prometheus、Grafana、Amazon CloudWatch、GCP Monitoring、Azure OpenTelemetry）</td>
</tr>
</tbody></table>
<h4 id="2-质量与风险管理（30-）"><a href="#2-质量与风险管理（30-）" class="headerlink" title="2. 质量与风险管理（30%）"></a><strong>2. 质量与风险管理（30%）</strong></h4><table>
<thead>
<tr>
<th>指标名称</th>
<th>定义与解释</th>
<th>权重</th>
<th>数据获取方式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>系统可用率（SLA）</strong></td>
<td>核心系统年度可用性达标率（如99.99%）</td>
<td>8%</td>
<td>Prometheus/Grafana监控报表</td>
</tr>
<tr>
<td><strong>MTTR - Time to Restore Service 平均故障修复时间</strong></td>
<td>生产环境故障从发生到恢复的平均时间（例：≤30分钟）</td>
<td>7%</td>
<td>Incident管理平台（如ServiceNow）报告</td>
</tr>
<tr>
<td><strong>安全漏洞修复时效</strong></td>
<td>高危漏洞从发现到修复的平均时间（如72小时内）</td>
<td>6%</td>
<td>漏洞扫描报告（如SonarQube、Qualys、Nessus）、Jira安全任务统计</td>
</tr>
<tr>
<td><strong>合规审计通过率</strong></td>
<td>通过监管审计（如SOX、GDPR）的条款覆盖率</td>
<td>5%</td>
<td>内外审计报告（Compliance部门）</td>
</tr>
<tr>
<td><strong>技术债务清理率</strong></td>
<td>年度清理技术债务（如遗留代码重构）的模块占比</td>
<td>4%</td>
<td>SonarQube技术债务报告</td>
</tr>
</tbody></table>
<h4 id="3-协作与知识传播（20-）"><a href="#3-协作与知识传播（20-）" class="headerlink" title="3. 协作与知识传播（20%）"></a><strong>3. 协作与知识传播（20%）</strong></h4><table>
<thead>
<tr>
<th>指标名称</th>
<th>定义与解释</th>
<th>权重</th>
<th>数据获取方式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>跨团队协作贡献度</strong></td>
<td>解决跨地域/部门协作问题的次数（如协调纽约与伦敦团队完成数据同步方案）</td>
<td>6%</td>
<td>Jira跨团队任务数量、Confluence协作文档</td>
</tr>
<tr>
<td><strong>知识文档复用率</strong></td>
<td>Confluence文档被其他项目引用的次数（如架构设计被10+项目参考）</td>
<td>5%</td>
<td>Confluence使用分析报告、Page浏览量</td>
</tr>
<tr>
<td><strong>内部培训覆盖率</strong></td>
<td>年度组织技术培训覆盖的团队成员比例（如80%参与云安全培训）</td>
<td>4%</td>
<td>HR/培训部门记录（如Learning Management System）</td>
</tr>
<tr>
<td><strong>开源社区贡献</strong></td>
<td>团队向金融科技开源项目（如Apache Fineract）提交的PR/Issue数量</td>
<td>3%</td>
<td>GitHub/GitLab开源项目贡献统计</td>
</tr>
<tr>
<td><strong>新人导师效能</strong></td>
<td>指导的新成员独立交付任务的周期缩短比例（如从3个月降至1.5个月）</td>
<td>2%</td>
<td>新人培训与交付统计报告（Mentorship计划跟踪）</td>
</tr>
</tbody></table>
<h4 id="4-效率与创新（15-）"><a href="#4-效率与创新（15-）" class="headerlink" title="4. 效率与创新（15%）"></a><strong>4. 效率与创新（15%）</strong></h4><table>
<thead>
<tr>
<th>指标名称</th>
<th>定义与解释</th>
<th>权重</th>
<th>数据获取方式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>CI/CD流水线成功率</strong></td>
<td>代码从提交到部署的成功率（如从90%提升至98%）</td>
<td>4%</td>
<td>Jenkins/GitLab CI/CD流水线报告</td>
</tr>
<tr>
<td><strong>AI工具采纳效率</strong></td>
<td>Copilot等工具节省的开发时间占比（如每日节省2小时）</td>
<td>3%</td>
<td>GitHub Copilot Analytics、IDE插件使用日志</td>
</tr>
<tr>
<td><strong>自动化测试覆盖率</strong></td>
<td>单元/集成测试覆盖的代码行数比例（如从70%提升至85%）</td>
<td>3%</td>
<td>SonarQube/Jacoco测试覆盖率报告</td>
</tr>
<tr>
<td><strong>创新提案落地数</strong></td>
<td>年度被采纳的技术创新方案（如区块链用于贸易金融）</td>
<td>3%</td>
<td>创新提案跟踪平台（Innovation Portal）</td>
</tr>
<tr>
<td><strong>专利/白皮书发布</strong></td>
<td>团队申请的金融科技专利数量或行业白皮书参与度</td>
<td>2%</td>
<td>法务部门专利申请记录、行业组织发布报告</td>
</tr>
<tr>
<td>在实施过程中，需关注以下几个关键挑战：</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>• <strong>数据质量问题</strong>：定期审查和校验数据的准确性，避免因数据偏差导致绩效不公平。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>• <strong>AI生成代码风险管理</strong>：建立专门的审查流程，防止AI生成代码带来的安全和合规风险。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>• <strong>团队文化适应性</strong>：提前沟通，让团队充分理解新绩效考核方式的价值，避免出现阻力和误解。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>reference link : <a target="_blank" rel="noopener" href="https://dora.dev/capabilities/">https://dora.dev/capabilities/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuang-weiming.github.io/2025/03/29/%E5%9F%BA%E4%BA%8EDORA%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB%E7%9A%84%E7%BB%A9%E6%95%88%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Weiming Zhuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuang's Diary">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/03/29/%E5%9F%BA%E4%BA%8EDORA%E6%8C%87%E6%A0%87%E4%BD%93%E7%B3%BB%E7%9A%84%E7%BB%A9%E6%95%88%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">基于DORA指标体系的绩效管理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-03-29 11:50:00 / Modified: 12:01:43" itemprop="dateCreated datePublished" datetime="2025-03-29T11:50:00+08:00">2025-03-29</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、战略层（DORA-Metrics）"><a href="#一、战略层（DORA-Metrics）" class="headerlink" title="一、战略层（DORA Metrics）"></a>一、战略层（DORA Metrics）</h2><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>部署频率（Deployment Frequency）</td>
<td>每月部署到生产环境的次数</td>
<td>≥ 每周一次</td>
</tr>
<tr>
<td>变更前置时间（Lead Time for Changes）</td>
<td>从代码提交到生产环境部署的平均时间</td>
<td>≤ 1周</td>
</tr>
<tr>
<td>变更失败率（Change Failure Rate）</td>
<td>生产环境变更导致事故的比例</td>
<td>≤ 5%</td>
</tr>
<tr>
<td>平均恢复时间（Mean Time to Restore，MTTR）</td>
<td>故障发生到修复完成的平均时长</td>
<td>≤ 2小时</td>
</tr>
</tbody></table>
<h2 id="二、执行层（精细化指标体系）"><a href="#二、执行层（精细化指标体系）" class="headerlink" title="二、执行层（精细化指标体系）"></a>二、执行层（精细化指标体系）</h2><h3 id="（一）研发阶段（Development）"><a href="#（一）研发阶段（Development）" class="headerlink" title="（一）研发阶段（Development）"></a>（一）研发阶段（Development）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>单元测试覆盖率</td>
<td>单元测试代码覆盖率</td>
<td>≥80%</td>
</tr>
<tr>
<td>代码审查一次通过率</td>
<td>首次通过代码审查比例</td>
<td>≥85%</td>
</tr>
<tr>
<td>技术债务</td>
<td>SonarQube技术债务评分</td>
<td>≤ 技术债务比例15%</td>
</tr>
<tr>
<td>敏捷迭代按时交付率</td>
<td>Sprint内任务完成率</td>
<td>≥90%</td>
</tr>
</tbody></table>
<h3 id="（二）部署阶段（Deployment）"><a href="#（二）部署阶段（Deployment）" class="headerlink" title="（二）部署阶段（Deployment）"></a>（二）部署阶段（Deployment）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>首次上线成功率</td>
<td>一次性成功部署比例</td>
<td>≥95%</td>
</tr>
<tr>
<td>部署自动化覆盖率</td>
<td>CI/CD管道自动化程度</td>
<td>≥90%</td>
</tr>
<tr>
<td>平均部署周期</td>
<td>提交到上线部署的周期</td>
<td>≤2天</td>
</tr>
</tbody></table>
<h3 id="（三）运维阶段（Operation）"><a href="#（三）运维阶段（Operation）" class="headerlink" title="（三）运维阶段（Operation）"></a>（三）运维阶段（Operation）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>系统可用率</td>
<td>系统正常运行的比例</td>
<td>≥99.95%</td>
</tr>
<tr>
<td>平均故障间隔（MTBF）</td>
<td>系统两次事故之间的平均时间</td>
<td>≥30天</td>
</tr>
<tr>
<td>自动化监控覆盖率</td>
<td>关键系统自动监控覆盖程度</td>
<td>≥95%</td>
</tr>
<tr>
<td>服务响应延迟</td>
<td>应用的99%响应延迟</td>
<td>≤500ms</td>
</tr>
</tbody></table>
<h3 id="（四）事故管理（Incident-Management）"><a href="#（四）事故管理（Incident-Management）" class="headerlink" title="（四）事故管理（Incident Management）"></a>（四）事故管理（Incident Management）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>事故响应时间</td>
<td>事故发生到首次响应的平均时间</td>
<td>≤15分钟</td>
</tr>
<tr>
<td>事故关闭时间（MTTR）</td>
<td>从事故发生到关闭的平均时间</td>
<td>≤2小时</td>
</tr>
<tr>
<td>一级事故数量</td>
<td>严重事故（P1级）发生次数</td>
<td>每月≤2次</td>
</tr>
<tr>
<td>RCA完成率</td>
<td>重大事故根因分析完成比例</td>
<td>100%</td>
</tr>
</tbody></table>
<h3 id="（五）变更管理（Change-Management）"><a href="#（五）变更管理（Change-Management）" class="headerlink" title="（五）变更管理（Change Management）"></a>（五）变更管理（Change Management）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>变更回退率</td>
<td>需要回滚的变更比例</td>
<td>≤5%</td>
</tr>
<tr>
<td>紧急变更比例</td>
<td>紧急变更占全部变更的比例</td>
<td>≤10%</td>
</tr>
<tr>
<td>变更审批效率</td>
<td>变更申请到审批完成时间</td>
<td>≤1个工作日</td>
</tr>
</tbody></table>
<h3 id="（六）系统淘汰管理（Demise-amp-Decommission）"><a href="#（六）系统淘汰管理（Demise-amp-Decommission）" class="headerlink" title="（六）系统淘汰管理（Demise &amp; Decommission）"></a>（六）系统淘汰管理（Demise &amp; Decommission）</h3><table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>目标值</th>
</tr>
</thead>
<tbody><tr>
<td>系统及时退役率</td>
<td>按计划淘汰系统的及时率</td>
<td>100%</td>
</tr>
<tr>
<td>淘汰成本控制</td>
<td>实际退役成本与计划的差异比例</td>
<td>≤10%</td>
</tr>
</tbody></table>
<h2 id="三、实施与治理模式"><a href="#三、实施与治理模式" class="headerlink" title="三、实施与治理模式"></a>三、实施与治理模式</h2><h3 id="1-报告与回顾机制"><a href="#1-报告与回顾机制" class="headerlink" title="1. 报告与回顾机制"></a>1. 报告与回顾机制</h3><ul>
<li>每季度向战略层汇报DORA指标，审视整体战略目标达成情况。</li>
<li>每月运营回顾精细化指标，针对执行偏差提出整改。</li>
<li>每周团队自查具体指标，持续改进。</li>
</ul>
<h3 id="2-技术支撑平台"><a href="#2-技术支撑平台" class="headerlink" title="2. 技术支撑平台"></a>2. 技术支撑平台</h3><ul>
<li>DevOps平台自动收集开发、测试、部署数据。</li>
<li>ITSM（如ServiceNow）跟踪事故、变更数据。</li>
<li>APM（如Prometheus/Grafana）实时监控运维指标。</li>
<li>数据可视化平台（如Power BI）集中展现指标。</li>
</ul>
<h3 id="3-组织与激励"><a href="#3-组织与激励" class="headerlink" title="3. 组织与激励"></a>3. 组织与激励</h3><ul>
<li>指标与团队绩效激励直接关联，优秀团队给予奖励。</li>
<li>存在明显问题的团队，组织针对性改进辅导。</li>
</ul>
<h2 id="四、预期效果"><a href="#四、预期效果" class="headerlink" title="四、预期效果"></a>四、预期效果</h2><ul>
<li>保持开发与运维的效率、稳定性双平衡。</li>
<li>确保监管合规性，同时保证技术与业务的快速响应能力。</li>
<li>促进组织持续改善，实现稳定高效的整体技术管理水平。</li>
</ul>
<h3 id="Reference-link"><a href="#Reference-link" class="headerlink" title="Reference link:"></a>Reference link:</h3><p><a target="_blank" rel="noopener" href="https://dora.dev/capabilities/">https://dora.dev/capabilities/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuang-weiming.github.io/2025/02/13/SFT%E3%80%81DPO%E3%80%81GRPO%E4%B8%89%E7%A7%8D%E8%AE%AD%E7%BB%83%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Weiming Zhuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuang's Diary">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/13/SFT%E3%80%81DPO%E3%80%81GRPO%E4%B8%89%E7%A7%8D%E8%AE%AD%E7%BB%83%E5%99%A8%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/" class="post-title-link" itemprop="url">SFT、DPO、GRPO三种训练器的训练数据与应用场景</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-13 18:22:00" itemprop="dateCreated datePublished" datetime="2025-02-13T18:22:00+08:00">2025-02-13</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2025-02-17 18:21:32" itemprop="dateModified" datetime="2025-02-17T18:21:32+08:00">2025-02-17</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="结论总结："><a href="#结论总结：" class="headerlink" title="结论总结："></a>结论总结：</h2><table>
<thead>
<tr>
<th>模型名称</th>
<th>SFTTrainer - Supervised Fine-Tuning Trainer - 监督式微调训练器</th>
<th>DPOTrainer - Direct Preference Optimization Trainer - 直接偏好优化训练器</th>
<th>GRPOTrainer - Generative Reward Policy Optimization Trainer - 生成式奖励策略优化训练器</th>
</tr>
</thead>
<tbody><tr>
<td>训练目标</td>
<td>模仿训练数据</td>
<td>对齐人类偏好</td>
<td>最大化奖励函数</td>
</tr>
<tr>
<td>数据需求</td>
<td>输入-输出数据对。数据形式是 “指令 -&gt; 期望输出” 的对应关系</td>
<td>偏好数据。”指令 -&gt; (偏好输出, 非偏好输出)” 的成对比较</td>
<td>奖励信号。数据形式是一个数值奖励，用于评价模型在环境中的行为</td>
</tr>
<tr>
<td>核心算法</td>
<td>监督学习 (交叉熵损失)</td>
<td>直接偏好优化 (DPO 损失)</td>
<td>强化学习 (PPO 算法)</td>
</tr>
<tr>
<td>优势</td>
<td>简单易用, 高效, 适用多种任务</td>
<td>更符合人类偏好, 避免奖励函数设计难题, 训练稳定, 对奖励函数偏差更鲁棒</td>
<td>直接优化目标指标, 可学习复杂策略, 适用于与环境交互任务, 精细行为控制</td>
</tr>
<tr>
<td>劣势</td>
<td>可能放大数据偏差, 难处理复杂偏好, 可能过拟合</td>
<td>需要偏好数据, 对偏好数据质量敏感, 可能牺牲部分生成能力</td>
<td>训练复杂不稳定, 奖励函数设计困难, 计算成本高, 可能奖励函数偏移</td>
</tr>
<tr>
<td>复杂度</td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
<tr>
<td>应用场景示例</td>
<td>- 内容生成: 自动生成产品描述、新闻稿、社交媒体文案等。<br>- 指令跟随: 简单的问答系统、文档摘要、代码生成等。 <br>- 数据增强: 生成特定格式或风格的合成数据，例如特定风格的文本或代码。<br></td>
<td>- 对话系统: 训练客服机器人、聊天机器人，使其回复更礼貌、更人性化、更符合用户期望。<br>- 内容审核: 训练模型判断文本是否安全、无害、符合道德标准。<br>- 偏好排序: 训练模型根据用户偏好对多个选项进行排序或选择 (例如，排序新闻摘要、推荐商品)。</td>
<td>- 游戏 AI: 训练游戏 Bot，在游戏中获得高分或战胜对手。<br>- 交易策略: 训练交易机器人，使其在股票市场或加密货币市场中最大化收益。<br>- 机器人控制: 训练机器人完成复杂任务，例如导航、物体抓取等，最大化任务完成效率或成功率。 <br>- 复杂对话策略: 训练对话系统进行多轮对话，最终达成用户目标 (例如，预定餐厅、解决复杂问题)。</td>
</tr>
</tbody></table>
<h2 id="1-输入-输出数据对-Input-Output-Data-Pairs-SFTTrainer-使用"><a href="#1-输入-输出数据对-Input-Output-Data-Pairs-SFTTrainer-使用" class="headerlink" title="1. 输入-输出数据对 (Input-Output Data Pairs) - SFTTrainer 使用"></a>1. 输入-输出数据对 (Input-Output Data Pairs) - SFTTrainer 使用</h2><p>这种数据形式是最直接的，用于监督式微调 (SFTTrainer)。 每个数据样本都包含一个 输入 (Input) 和一个期望的 输出 (Output)。</p>
<p>应用场景例子：<strong>指令跟随 (Instruction Following)</strong> - 简单的问答任务<br><strong>示例：</strong></p>
<ul>
<li>问答系统： “问题 -&gt; 答案”</li>
<li>翻译任务： “原文 -&gt; 译文”</li>
<li>摘要生成： “文章 -&gt; 摘要”</li>
<li>样例数据格式 (JSON 格式示例):<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span>: <span class="string">&quot;法国的首都是哪里？&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output&quot;</span>: <span class="string">&quot;法国的首都是巴黎。&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span>: <span class="string">&quot;请写一个关于夏天的简短故事。&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output&quot;</span>: <span class="string">&quot;阳光洒在金色的沙滩上，海风轻轻吹拂，孩子们在海边嬉戏，冰淇淋融化在甜甜的笑容里，夏天真美好。&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span>: <span class="string">&quot;将这句话翻译成英文：你好世界。&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;output&quot;</span>: <span class="string">&quot;Hello world.&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// ... 更多数据样本</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
解释:</li>
<li>“instruction” (输入): 代表用户给模型的指令或问题。</li>
<li>“output” (输出): 代表模型应该生成的期望回复或答案。</li>
<li>数据目标: SFTTrainer 的目标是让模型学习将 “instruction” 映射到 “output”，模仿训练数据中的这种对应关系。</li>
</ul>
<p>应用场景例子：<strong>内容生成 (Content Generation)</strong> - 生成产品描述</p>
<ul>
<li>样例数据格式 (JSON 格式示例):<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;input&quot;</span>: &#123;</span><br><span class="line">      <span class="attr">&quot;product_name&quot;</span>: <span class="string">&quot;智能咖啡机&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;features&quot;</span>: [<span class="string">&quot;一键操作&quot;</span>, <span class="string">&quot;多种咖啡模式&quot;</span>, <span class="string">&quot;可预约&quot;</span>, <span class="string">&quot;自动清洗&quot;</span>],</span><br><span class="line">      <span class="attr">&quot;materials&quot;</span>: [<span class="string">&quot;不锈钢&quot;</span>, <span class="string">&quot;耐热玻璃&quot;</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">&quot;output&quot;</span>: <span class="string">&quot;这款智能咖啡机让您在家也能轻松享受咖啡馆级的美味。一键操作，多种咖啡模式随心选择，更有预约功能，让您早晨醒来就能品尝到香浓咖啡。采用不锈钢和耐热玻璃材质，坚固耐用，并具备自动清洗功能，省心省力。&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">// ... 更多数据样本</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>
解释:</li>
<li>“input” (输入): 可以是更结构化的信息，例如产品的特征、材质等。</li>
<li>“output” (输出): 是基于输入信息生成的期望产品描述文本。</li>
</ul>
<h2 id="2-偏好数据-Pairwise-Ranking-DPOTrainer-使用"><a href="#2-偏好数据-Pairwise-Ranking-DPOTrainer-使用" class="headerlink" title="2. 偏好数据 (Pairwise Ranking) - DPOTrainer 使用"></a>2. 偏好数据 (Pairwise Ranking) - DPOTrainer 使用</h2><p>这种数据形式用于直接偏好优化 (DPOTrainer)。 对于同一个输入，我们提供两个模型生成的输出，并标注哪个输出更符合偏好。</p>
<p>应用场景例子：<strong>对话系统 (Chatbot)</strong> - 提升回复质量和偏好</p>
<ul>
<li>样例数据格式 (JSON 格式示例):<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span>: <span class="string">&quot;今天天气怎么样？&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;chosen&quot;</span>: <span class="string">&quot;今天天气晴朗，阳光明媚，非常适合户外活动。&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;rejected&quot;</span>: <span class="string">&quot;天气还行。&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">&quot;instruction&quot;</span>: <span class="string">&quot;请问你能推荐一家附近的意大利餐厅吗？&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;chosen&quot;</span>: <span class="string">&quot;当然，附近有一家评价很高的意大利餐厅，名叫“托斯卡纳阳光”，他们家的披萨和意面非常受欢迎，地址是… [地址信息] …，您要我帮您查询一下电话或者预定吗？&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;rejected&quot;</span>: <span class="string">&quot;我推荐一家意大利餐厅。&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="comment">// ... 更多数据样本</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li>解释:<ul>
<li>“instruction” (输入): 用户的问题或指令。</li>
<li>“chosen” (偏好输出): 被认为更好或更符合偏好的回复。例如，更详细、更礼貌、更乐于助人的回复。</li>
<li>“rejected” (非偏好输出): 被认为相对较差或不太符合偏好的回复。例如，更简短、更生硬、信息量较少的回复。</li>
<li>数据目标: DPOTrainer 学习到，对于相同的 “instruction”，模型应该倾向于生成类似 “chosen” 这样的回复，而不是 “rejected” 这样的回复。偏好可以是基于礼貌程度、信息量、是否乐于助人、是否符合特定价值观等等。</li>
</ul>
</li>
</ul>
<h2 id="3-奖励信号-Reward-Signal-GRPOTrainer-使用"><a href="#3-奖励信号-Reward-Signal-GRPOTrainer-使用" class="headerlink" title="3. 奖励信号 (Reward Signal) - GRPOTrainer 使用"></a>3. 奖励信号 (Reward Signal) - GRPOTrainer 使用</h2><p>奖励信号是一个数值，用于评价模型在特定环境或任务中生成的输出质量。 GRPOTrainer 使用强化学习方法，目标是最大化模型获得的累积奖励。</p>
<p>应用场景例子：<strong>游戏 AI (Game AI)</strong> - 训练游戏 Bot 下围棋</p>
<ul>
<li>奖励函数示例 (Python 伪代码):<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reward_function</span>(<span class="params">game_state, action</span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">  定义围棋游戏中的奖励函数.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    game_state: 当前的棋局状态.</span></span><br><span class="line"><span class="string">    action: 模型采取的落子动作.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    reward:  一个数值奖励信号.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> is_illegal_move(game_state, action): <span class="comment"># 落子是否为非法</span></span><br><span class="line">    reward = <span class="number">-10</span>  <span class="comment"># 非法落子，负奖励，惩罚模型</span></span><br><span class="line">  <span class="keyword">elif</span> is_capture_opponent_piece(game_state, action): <span class="comment"># 是否吃掉对方棋子</span></span><br><span class="line">    reward = +<span class="number">5</span>   <span class="comment"># 吃掉对方棋子，正奖励</span></span><br><span class="line">  <span class="keyword">elif</span> is_win(game_state): <span class="comment"># 是否赢得游戏</span></span><br><span class="line">    reward = +<span class="number">100</span>  <span class="comment"># 赢得游戏，巨大正奖励</span></span><br><span class="line">  <span class="keyword">elif</span> is_lose(game_state): <span class="comment"># 是否输掉游戏</span></span><br><span class="line">    reward = <span class="number">-50</span>  <span class="comment"># 输掉游戏，负奖励</span></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    reward = <span class="number">-0.1</span> <span class="comment"># 常规落子，轻微负奖励 (鼓励尽快结束游戏，避免无意义的步骤 - 可根据实际情况调整)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> reward</span><br></pre></td></tr></table></figure></li>
<li>解释:<ul>
<li>奖励函数: <code>reward_function</code> 就是一个奖励函数，它根据当前的游戏状态和模型采取的动作，计算出一个数值奖励。</li>
<li>奖励信号: 每次模型在游戏中执行一个动作后，环境 (围棋游戏) 会根据 <code>reward_function</code> 计算出一个奖励值，并将这个奖励值反馈给模型。</li>
<li>数据目标: GRPOTrainer 通过不断尝试不同的动作，并根据获得的奖励信号学习，目标是找到一个策略 (即模型的参数)，使得在围棋游戏中能够获得尽可能高的累积奖励 (例如，最终赢得游戏)。</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuang-weiming.github.io/2025/02/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A6%81%E5%8D%A0%E7%94%A8%E5%A4%9A%E5%B0%91%E6%98%BE%E5%AD%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Weiming Zhuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuang's Diary">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A6%81%E5%8D%A0%E7%94%A8%E5%A4%9A%E5%B0%91%E6%98%BE%E5%AD%98/" class="post-title-link" itemprop="url">大模型要占用多少显存</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-02-11 09:48:00" itemprop="dateCreated datePublished" datetime="2025-02-11T09:48:00+08:00">2025-02-11</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2025-02-27 10:52:31" itemprop="dateModified" datetime="2025-02-27T10:52:31+08:00">2025-02-27</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="部署要占用多少显存"><a href="#部署要占用多少显存" class="headerlink" title="部署要占用多少显存"></a>部署要占用多少显存</h2><p>以运行精度为 INT8 的大模型为例，这种精度的参数，一个参数需要占用一个字节。<br>$1B参数模型 = 10亿参数 * 每个参数占用1Byte$<br>$1G显存 = 1024<em>1024</em>1024Byte$<br>也就是说<br><strong>INT8 精度类型：1B 参数需要约 1G 显存。</strong></p>
<table>
<thead>
<tr>
<th>dtype</th>
<th align="center">1B模型需要占用的显存</th>
</tr>
</thead>
<tbody><tr>
<td>float32</td>
<td align="center">4G</td>
</tr>
<tr>
<td>fp16/bf16</td>
<td align="center">2G</td>
</tr>
<tr>
<td>int8</td>
<td align="center">1G</td>
</tr>
<tr>
<td>int4</td>
<td align="center">0.5G</td>
</tr>
<tr>
<td>然后就可以快速计算各个类型精度的大模型需要多少显存，例如 f16 的 70B 参数大模型，就需要“精度膨胀系数” 2*70=140G显存。</td>
<td align="center"></td>
</tr>
</tbody></table>
<h2 id="训练要占用多少显存"><a href="#训练要占用多少显存" class="headerlink" title="训练要占用多少显存"></a>训练要占用多少显存</h2><p>这里还有另外一个在线的网页工具：<a target="_blank" rel="noopener" href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">https://huggingface.co/spaces/hf-accelerate/model-memory-usage</a><br><img src="/2025/02/11/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A6%81%E5%8D%A0%E7%94%A8%E5%A4%9A%E5%B0%91%E6%98%BE%E5%AD%98/1.jpg"></p>
<p>模型包括： DeepSeek R1 (671B)，DeepSeek R1 Distill Qwen 7B， 14B， 32B， 以及 DeepSeek R1 Distill Llama 8B， 70B 这些模型，</p>
<ol>
<li>假设在并发 3 个用户的情况下运行这些模型所需的资源？</li>
<li>关于 DeepSeek R1 系列模型在不同数据量下进行微调（并非全量训练），并在一天内完成微调所需的硬件资源。分别在 1K，1M，1GB，1T 数据量下训练这些模型所需的资源和训练时长。</li>
</ol>
<p>根据您提供的信息，以及之前的分析，以下是关于 DeepSeek R1 系列模型微调硬件资源需求的全面分析，并以表格形式呈现：</p>
<h2 id="DeepSeek-R1-模型微调硬件资源需求总表-单日内完成"><a href="#DeepSeek-R1-模型微调硬件资源需求总表-单日内完成" class="headerlink" title="DeepSeek R1 模型微调硬件资源需求总表 (单日内完成)"></a>DeepSeek R1 模型微调硬件资源需求总表 (单日内完成)</h2><p>采用BF16（2字节/参数）的情况下，考虑到 Unsloth 主要优化 LoRA 微调，并且更适用于单 GPU 或少量 GPU 场景，基于 Unsloth 优化 LoRA 微调的假设，重新评估 DeepSeek R1 系列模型在中小数据量 (1K, 1M, 1G, 10G) 下的硬件资源需求。 对于超大数据量 (100G) 和超大模型 (DeepSeek-R1-671B)，我仍然保留 ZeRO-3 优化。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>数据size</th>
<th>GPU 配置 (显存需求)</th>
<th>CPU 核心数</th>
<th>内存</th>
<th>存储 (SSD/NVMe)</th>
<th>网络带宽</th>
<th>关键配置说明</th>
</tr>
</thead>
<tbody><tr>
<td><strong>DeepSeek-R1-671B BF16</strong></td>
<td>1M</td>
<td>16×A100 80G</td>
<td>64核</td>
<td>512GB</td>
<td>2TB</td>
<td>25Gbps RDMA</td>
<td>模型并行 (TP=16) + 数据并行 (DP=2)</td>
</tr>
<tr>
<td></td>
<td>1G</td>
<td>64×A100 80G</td>
<td>256核</td>
<td>1TB</td>
<td>5TB</td>
<td>50Gbps RDMA</td>
<td>TP=8 + DP=8 + ZeRO-3</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>128×A100 80G</td>
<td>512核</td>
<td>2TB</td>
<td>10TB</td>
<td>100Gbps RDMA</td>
<td>TP=8 + DP=16 + ZeRO-3</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>192×A100 80G</td>
<td>768核</td>
<td>4TB</td>
<td>15TB</td>
<td>150Gbps RDMA</td>
<td>TP=8 + DP=24 + ZeRO-3</td>
</tr>
<tr>
<td><strong>Distill-Qwen-7B 4-bit</strong></td>
<td>1M</td>
<td>1×A10G 24G (Unsloth)</td>
<td>8核</td>
<td>64GB</td>
<td>500GB</td>
<td>1Gbps</td>
<td>单卡 4-bit LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>1G</td>
<td>2×A10G 24G (Unsloth)</td>
<td>16核</td>
<td>128GB</td>
<td>1TB</td>
<td>5Gbps</td>
<td>DP=2 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>4×A10G 24G (Unsloth)</td>
<td>32核</td>
<td>256GB</td>
<td>2TB</td>
<td>5Gbps</td>
<td>DP=4 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>8×A10G 24G (Unsloth)</td>
<td>64核</td>
<td>512GB</td>
<td>3TB</td>
<td>10Gbps</td>
<td>DP=8 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td><strong>Distill-Qwen-14B 4-bit</strong></td>
<td>1M</td>
<td>1×A100 40G (Unsloth)</td>
<td>16核</td>
<td>128GB</td>
<td>1TB</td>
<td>5Gbps</td>
<td>单卡 LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>1G</td>
<td>4×A100 40G (Unsloth)</td>
<td>32核</td>
<td>256GB</td>
<td>2TB</td>
<td>10Gbps</td>
<td>DP=4 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>8×A100 40G (Unsloth)</td>
<td>64核</td>
<td>512GB</td>
<td>3TB</td>
<td>10Gbps</td>
<td>DP=8 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>16×A100 40G (Unsloth)</td>
<td>128核</td>
<td>1TB</td>
<td>5TB</td>
<td>25Gbps</td>
<td>DP=16 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td><strong>Distill-Qwen-32B BF16</strong></td>
<td>1G</td>
<td>4×A100 80G (Unsloth)</td>
<td>64核</td>
<td>512GB</td>
<td>2TB</td>
<td>10Gbps</td>
<td>TP=4 + DP=2</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>8×A100 80G (Unsloth)</td>
<td>128核</td>
<td>1TB</td>
<td>3TB</td>
<td>25Gbps</td>
<td>DP=8 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>32×A100 80G</td>
<td>256核</td>
<td>2TB</td>
<td>10TB</td>
<td>50Gbps RDMA</td>
<td>TP=4 + DP=8 + ZeRO-3</td>
</tr>
<tr>
<td><strong>Distill-Llama-8B 4-bit</strong></td>
<td>1M</td>
<td>1×A10G 24G (Unsloth)</td>
<td>8核</td>
<td>64GB</td>
<td>500GB</td>
<td>1Gbps</td>
<td>单卡 LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>1G</td>
<td>2×A10G 24G (Unsloth)</td>
<td>16核</td>
<td>128GB</td>
<td>1TB</td>
<td>5Gbps</td>
<td>DP=2 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>4×A10G 24G (Unsloth)</td>
<td>32核</td>
<td>256GB</td>
<td>2TB</td>
<td>5Gbps</td>
<td>DP=4 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>8×A10G 24G (Unsloth)</td>
<td>48核</td>
<td>384GB</td>
<td>3TB</td>
<td>10Gbps</td>
<td>DP=8 + LoRA 微调 (Unsloth)</td>
</tr>
<tr>
<td><strong>Distill-Llama-70B BF16</strong></td>
<td>1M</td>
<td>8×A100 80G</td>
<td>64核</td>
<td>512GB</td>
<td>2TB</td>
<td>25Gbps RDMA</td>
<td>TP=4 + DP=2 (ZeRO-3 for 70B still recommended)</td>
</tr>
<tr>
<td></td>
<td>1G</td>
<td>32×A100 80G</td>
<td>128核</td>
<td>1TB</td>
<td>5TB</td>
<td>25Gbps RDMA</td>
<td>TP=4 + DP=8 (ZeRO-3 for 70B still recommended)</td>
</tr>
<tr>
<td></td>
<td>10G</td>
<td>64×A100 80G</td>
<td>256核</td>
<td>2TB</td>
<td>10TB</td>
<td>50Gbps RDMA</td>
<td>TP=8 + DP=8 + ZeRO-3</td>
</tr>
<tr>
<td></td>
<td>100G</td>
<td>128×A100 80G</td>
<td>512核</td>
<td>4TB</td>
<td>15TB</td>
<td>100Gbps RDMA</td>
<td>TP=8 + DP=16 + ZeRO-3</td>
</tr>
</tbody></table>
<p><strong>GPU 配置 (显存需求)</strong>: 指完成微调任务所需的 GPU 型号和数量，以及总显存需求。例如 “16×A100 80G” 表示需要 16 张 80GB 显存的 A100 GPU。<br><strong>CPU 核心数</strong>: 训练任务所需的 CPU 核心数量，主要用于数据预处理和模型管理。<br><strong>内存</strong>: 系统内存需求，用于数据缓存、模型加载和训练过程中的临时数据存储。<br><strong>存储 (SSD/NVMe)</strong>: 高速固态硬盘容量需求，用于存储训练数据、模型参数和中间结果。NVMe SSD 提供更快的读写速度，适用于大数据量训练。<br><strong>网络带宽</strong>: 多 GPU 训练时所需的网络带宽，用于 GPU 之间的数据通信。RDMA (Remote Direct Memory Access) 网络提供更低的延迟和更高的带宽，适用于大规模分布式训练。<br><strong>关键配置说明</strong>: 简要描述了针对不同模型和数据量所采用的关键并行策略和优化技术，例如模型并行 (TP)、数据并行 (DP)、ZeRO-3 优化和梯度累积等。<br><strong>预估并发 3 用户显存需求</strong>: 粗略估计为模型参数显存占用的 3 倍或更高。因为并发用户需要模型的多份副本或共享模型但需要额外的上下文缓存等。实际情况可能更复杂。</p>
<ol>
<li><strong>Unsloth 对资源需求的影响</strong>:<ul>
<li><strong>中小模型 (Distill-Qwen-7B/14B/32B, Distill-Llama-8B) 和中小数据量 (1K - 100G)</strong>: 使用 Unsloth 库进行 LoRA 微调，可以显著降低 GPU 资源需求。例如，Distill-Qwen-7B 在 1K 数据量下，单张 A10G 24G 即可完成微调；即使在 100G 数据量下，也仅需 8 张 A10G 24G GPU。</li>
<li><strong>超大模型 (DeepSeek-R1-671B) 和大数据量 (100G)</strong>: 对于这些极端情况，Unsloth 的优化可能不足以完全解决显存瓶颈。因此，表格中仍然保留了 ZeRO-3 优化，并结合 Unsloth 的 FlashAttention-2 优化，以期达到最佳的性能和资源效率。</li>
</ul>
</li>
<li><strong>LoRA 微调的优势</strong>:<ul>
<li>表格中所有基于 Unsloth 的配置方案都假设使用 LoRA 微调。LoRA 本身就是一种参数高效微调方法，可以显著减少需要训练的参数量，从而降低显存需求和加速训练。</li>
<li>Unsloth 进一步优化了 LoRA 的实现，使其在速度和显存效率方面更具优势。</li>
</ul>
</li>
<li><strong>FlashAttention-2 的加速作用</strong>:<ul>
<li>Unsloth 集成的 FlashAttention-2 可以显著加速训练过程，这有助于在 “日内完成微调” 的目标下，使用更少的 GPU 资源。</li>
</ul>
</li>
</ol>
<h2 id="关键分析逻辑"><a href="#关键分析逻辑" class="headerlink" title="关键分析逻辑"></a>关键分析逻辑</h2><ol>
<li>模型参数量 vs. GPU 显存:<ul>
<li>参数高效<strong>微调 (如 LoRA) 显著降低了显存需求</strong>，公式估算如下： $$ \text{显存} \approx \text{模型参数} \times (2\ \text{bytes} \times \text{激活系数}) + \text{优化器状态} $$ 其中，激活系数在 LoRA 场景下约为 0.1-0.3，优化器状态通过 ZeRO-3 等技术可以大幅减少。</li>
</ul>
</li>
<li>数据量 vs. 训练并行度:<ul>
<li>小数据量 (1K, 1M): 资源需求主要由模型大小决定。较小的 Distill 模型甚至可以在单张消费级 GPU 上完成微调。</li>
<li>大数据量 (1G, 1T): 数据并行成为关键。需要增加 GPU 数量以提高数据吞吐量，并配合高速网络 (如 RDMA) 保证并行效率。</li>
</ul>
</li>
<li>训练时长与硬件资源:<ul>
<li>表格中的配置旨在将微调时间压缩到 <strong>一天以内</strong>。更快的训练速度通常需要更多的 GPU 资源并行计算。</li>
<li>实际训练时间还会受到 <strong>模型结构</strong>、<strong>超参数设置</strong>、<strong>优化算法</strong> 等多种因素影响。上述表格提供的是一个 <strong>硬件配置参考</strong>，实际部署时可能需要根据具体情况进行调整。</li>
</ul>
</li>
<li>优化技术:<ul>
<li>混合精度训练 (BF16/FP16): 降低显存占用和计算复杂度，加速训练过程。</li>
<li>梯度检查点: 通过计算换取显存，进一步降低显存峰值。</li>
<li>ZeRO-3: 将优化器状态、梯度和模型参数分片到多张 GPU 上，极大地减少单卡显存需求，尤其适用于超大模型 (如 671B)。</li>
<li>模型并行 (Tensor Parallelism, TP): 将模型按层或张量切分到多张 GPU 上，降低单卡显存压力，适用于超大模型。</li>
<li>数据并行 (Data Parallelism, DP): 将数据分片到多张 GPU 上，每张 GPU 训练模型完整副本的一部分数据，提高数据吞吐量，适用于大数据量训练。</li>
<li>梯度累积: 在显存受限时，通过多次小批量梯度计算累积梯度，模拟大批量训练的效果。</li>
</ul>
</li>
<li>网络与存储:<ul>
<li><strong>RDMA 网络</strong>: 对于大规模分布式训练 (尤其是模型并行和数据并行结合)，RDMA 网络可以显著降低 GPU 间通信延迟，提高并行效率。</li>
<li>高速 SSD/NVMe 存储: 大数据量训练时，高速存储可以加速数据加载，避免 I/O 瓶颈。</li>
</ul>
</li>
</ol>
<p><strong>请注意:</strong></p>
<ul>
<li>上述表格提供的是 <strong>估算和建议</strong>，实际资源需求可能因具体任务和实现细节有所不同。</li>
<li>在实际部署时，建议 <strong>从小规模配置开始测试</strong>，并根据训练速度和资源利用率逐步调整硬件配置。</li>
<li>云服务平台通常提供多种 GPU 实例和高速网络配置，可以根据表格中的建议选择合适的云资源进行模型微调。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://zhuang-weiming.github.io/2025/02/08/DeepSeek-R1%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Weiming Zhuang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhuang's Diary">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/02/08/DeepSeek-R1%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/" class="post-title-link" itemprop="url">DeepSeek-R1的核心技术</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-02-08 12:00:00 / Modified: 23:37:27" itemprop="dateCreated datePublished" datetime="2025-02-08T12:00:00+08:00">2025-02-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="DeepSeek-R1的实施步骤"><a href="#DeepSeek-R1的实施步骤" class="headerlink" title="DeepSeek-R1的实施步骤"></a>DeepSeek-R1的实施步骤</h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a>本身就是开源的，HuggingFace <a target="_blank" rel="noopener" href="https://github.com/huggingface/open-r1">Open R1 项目</a> ，  <a target="_blank" rel="noopener" href="https://github.com/simplescaling/s1">李飞飞团队s1项目</a> ， <a target="_blank" rel="noopener" href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a> 在部分复现DeepSeek R1，还有 <a target="_blank" rel="noopener" href="https://github.com/Jiayi-Pan/TinyZero">TinyZero 项目</a>在复现DeepSeek R1-Zero，又是为何？<br>根据 DeepSeek-R1 的技术报告，分3个步骤完成这个项目：</p>
<ul>
<li>第1步：用 DeepSeek-R1 蒸馏高质量语料库，来复制R1-Distill模型。</li>
<li>第2步：复制 DeepSeek (V3) 用来构建R1-Zero的纯强化学习（RL）pipeline。这可能涉及为数学、推理和代码整理新的大规模数据集。</li>
<li>第3步：通过多阶段训练，从基础模型过渡到RL版本。<br><img src="/2025/02/08/DeepSeek-R1%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/1.jpg"></li>
</ul>
<p>结合DeepSeek的官方技术报告来看，也就是说，Open R1项目首先要实现的，是用R1数据蒸馏小模型，看看效果是不是像DeepSeek说的那么好：</p>
<h2 id="DeepSeek-R1的实施效果"><a href="#DeepSeek-R1的实施效果" class="headerlink" title="DeepSeek-R1的实施效果"></a>DeepSeek-R1的实施效果</h2><p>DeepSeek开源了6个用R1蒸馏的小模型，其中蒸馏版Qwen-1.5甚至能在部分任务上超过GPT-4o。<br><img src="/2025/02/08/DeepSeek-R1%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/2.jpg"><br>接下来，就是按照DeepSeek所说，不用SFT，单纯依靠RL调教出R1-Zero，在R1-Zero的基础上复刻出性能逼近o1的R1模型。</p>
<p>其中R1技术报告讲到，DeepSeek-R1训练过程中引入了一个多阶段训练流程，具体包括以下4个阶段：</p>
<ol>
<li>冷启动<br> 用数千个长思维链（CoT）样本对基础模型进行监督微调（SFT），为模型提供初始的推理能力。</li>
<li>面向推理的强化学习<br> 在第一个SFT阶段的基础之上，用和训练R1-Zero相同的大规模强化学习方法，进一步提升模型的推理能力，特别是应对编程、数学、科学和逻辑推理任务的能力。</li>
<li>拒绝采样的监督微调<br> 再次使用监督微调（SFT），提升模型的非推理能力，如事实知识、对话能力等。</li>
<li>针对所有场景的强化学习<br> 这次强化学习的重点是让模型行为与人类偏好保持一致，提升模型的可用性和安全性。<h2 id="Open-R1做了什么？"><a href="#Open-R1做了什么？" class="headerlink" title="Open R1做了什么？"></a>Open R1做了什么？</h2>目前，在<a target="_blank" rel="noopener" href="https://github.com/huggingface/open-r1">open-r1 GitHub仓库</a>中，已经可以看到这几个文件：</li>
</ol>
<ul>
<li><p>GRPO（Grouped Relative Policy Optimization）实现，<code>grpo.py</code>: trains a model with GRPO on a given dataset.<br>  在 Open R1 发布后，GRPO已整合至TRL最新版本（<a href="https://link.zhihu.com/?target=https://x.com/QGallouedec/status/1884978284686905468">v0.14</a>，Jan 30, 2025）。该整合方案支持使用单个或多个奖励函数模型进行模型训练。GRPO 实现方案深度集成了 DeepSpeed ZeRO 1/2/3 分布式训练框架以实现多 GPU 扩展，并采用 vLLM 加速生成过程——这正是在线训练方法的主要性能瓶颈。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> GRPOConfig, GRPOTrainer</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;trl-lib/tldr&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dummy reward: rewards completions that are close to 20 characters</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reward_len</span>(<span class="params">completions, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [-<span class="built_in">abs</span>(<span class="number">20</span> - <span class="built_in">len</span>(completion)) <span class="keyword">for</span> completion <span class="keyword">in</span> completions]</span><br><span class="line"></span><br><span class="line">training_args = GRPOConfig(output_dir=<span class="string">&quot;Qwen2-0.5B-GRPO&quot;</span>, logging_steps=<span class="number">10</span>)</span><br><span class="line">trainer = GRPOTrainer(</span><br><span class="line">    model=<span class="string">&quot;Qwen/Qwen2-0.5B-Instruct&quot;</span>,</span><br><span class="line">    reward_funcs=reward_len,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br><span class="line"><span class="comment"># (Feb 2nd)仍存在显存占用过高的问题，我们正在通过性能剖析进行优化改进。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>合成数据生成器，<code>generate.py</code>: generates synthetic data from a model using <a target="_blank" rel="noopener" href="https://github.com/argilla-io/distilabel">Distilabel</a>.<br>  R1 技术报告中最引人注目的发现之一是：主模型可用于生成合成推理轨迹，而基于该数据集微调的较小模型可获得与主模型相近的性能提升。因此Open R1自然希望复现该合成推理数据集，使社区能够在其他模型上进行微调实验。<br>  面对 R1 这类超大模型，核心挑战在于高效扩展生成规模。Open R1花费一周时间尝试了多种配置方案：该模型可部署在 2 个 8xH100 节点（16 块 H100 GPU）上，我们最初基于此配置使用 vLLM 作为推理服务器。但很快发现该方案存在性能瓶颈：由于 GPU 的 KV 缓存快速耗尽，吞吐量未达最优且仅支持 8 路并行请求。当缓存耗尽时，占用大量缓存资源的请求会被抢占；若配置为<code>PreemptionMode.RECOMPUTE</code>模式，这些请求将在显存释放后重新调度。为此我们切换至 4x8xH100 节点配置（共 32 块 H100 GPU）。该方案为 32 路并行请求提供了充足的显存余量，基本避免了因 100% 缓存占用导致的请求重新调度问题。初始阶段我们采用批量请求查询 vLLM 服务器，但很快发现批次中的长尾样本会导致GPU利用率波动——新批次需等待前一批次最后一个样本完成后才能开始处理。将批量推理切换为流式处理后，GPU利用率显著稳定。<br><img src="/2025/02/08/DeepSeek-R1%E7%9A%84%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF/3.jpg"></p>
<p>  该优化仅需修改vLLM服务器的请求发送代码。批量推理代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># send requests in batches of 500</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> batch_generator(dataset, bs=<span class="number">500</span>):</span><br><span class="line">    active_tasks = []</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> batch:</span><br><span class="line">        task = asyncio.create_task(send_requests(row))</span><br><span class="line">        active_tasks.add(task)</span><br><span class="line">    <span class="keyword">if</span> active_tasks:</span><br><span class="line">        <span class="keyword">await</span> asyncio.gather(*active_tasks)</span><br></pre></td></tr></table></figure>
<p>  流式请求的新版代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">active_tasks = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="comment"># keep the total active requests under 500</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(active_tasks) &gt;= <span class="number">500</span>:</span><br><span class="line">        done, active_tasks = <span class="keyword">await</span> asyncio.wait(</span><br><span class="line">            active_tasks,</span><br><span class="line">            return_when=asyncio.FIRST_COMPLETED</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    task = asyncio.create_task(send_requests(row))</span><br><span class="line">    active_tasks.add(task)</span><br><span class="line"></span><br><span class="line"><span class="comment"># wait for all remaining tasks to complete</span></span><br><span class="line"><span class="keyword">if</span> active_tasks:</span><br><span class="line">    <span class="keyword">await</span> asyncio.gather(*active_tasks)</span><br><span class="line">    <span class="comment"># Open R1当前的生成速率已趋于稳定，但对于长查询被抢占时是否采用CPU 缓存策略仍需进一步探索。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>实施监督微调训练代码，<code>sft.py</code>: performs a simple SFT of a model on a dataset.</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Train via <span class="built_in">command</span> line</span></span><br><span class="line">accelerate launch --config_file=recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \</span><br><span class="line">    --model_name_or_path Qwen/Qwen2.5-1.5B-Instruct \</span><br><span class="line">    --dataset_name HuggingFaceH4/Bespoke-Stratos-17k \</span><br><span class="line">    --learning_rate 2.0e-5 \</span><br><span class="line">    --num_train_epochs 1 \</span><br><span class="line">    --packing \</span><br><span class="line">    --max_seq_length 4096 \</span><br><span class="line">    --per_device_train_batch_size 2 \</span><br><span class="line">    --gradient_accumulation_steps 8 \</span><br><span class="line">    --gradient_checkpointing \</span><br><span class="line">    --bf16 \</span><br><span class="line">    --output_dir data/Qwen2.5-1.5B-Open-R1-Distill</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Train via YAML config</span></span><br><span class="line">accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/openr1/sft.py \</span><br><span class="line">    recipes/Qwen/Qwen2.5-1.5B-Instruct/sft/config_demo.yaml</span><br></pre></td></tr></table></figure></li>
<li><p>训练和评估代码，<code>evaluate.py</code>: evaluates a model on the R1 benchmarks.</p>
</li>
</ul>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>社区在多个与R1相关的数据集项目上非常活跃，以下是一些亮点：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k">bespokelabs/Bespoke-Stratos-17k</a>：这是对 Berkeley Sky-T1 数据管线的复制，使用 DeepSeek-R1 创建一个包含问题、推理轨迹和答案的数据集。随后，这些数据被用于通过类似于 R1 论文中的蒸馏方法，微调 7B 和 32B 的 Qwen 模型。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k">open-thoughts/OpenThoughts-114k</a>：一个“开放的合成推理数据集，包含 114k 个高质量样本，涵盖数学、科学、代码和谜题”。这是 Open Thoughts 项目的一部分。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/cognitivecomputations/dolphin-r1">cognitivecomputations/dolphin-r1</a>：一个包含 80 万样本的数据集，样本来自 DeepSeek-R1、Gemini flash 以及来自 DolphinChat 的 20 万样本，目的是帮助训练 R1 风格的模型。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT">ServiceNow-AI/R1-Distill-SFT</a>：目前有 17,000 个样本，这是 ServiceNow 语言模型实验室为支持 Open-R1 工作而创建的数据集。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/NovaSky-AI/Sky-T1_data_17k">NovaSky-AI/Sky-T1_data_17k</a>：用于训练 Sky-T1-32B-Preview 的 17k 训练数据。最终数据包含来自 APPs 和 TACO 的 5k 编码数据，以及来自 NuminaMATH 数据集的 AIME、MATH 和 Olympiads 子集的 10k 数学数据。此外，我们还维护了来自 STILL-2 的 1k 科学和拼图数据。使用该数据集训练的模型成本不到 450 美元。</li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B">Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B</a>：这个数据集扩展了 <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2406.08464">Magpie</a> 和方法，通过生成没有起始提示的指令数据来包括推理过程。指令由 Llama 3.1 70B Instruct 和 Llama 3.3 70B Instruct 生成，响应则由 DeepSeek-R1-Distill-Llama-70B 生成。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li>SFT后，进步显著，怎么做到的？<br> 一是微调用的训练数据起到了一定作用；二是强制让模型延长思考时间（test time scaling），具体做法叫做（Budget Forcing）预算强制，也就是强制限制模型使用最大或最小 tokens 进行推理，以此控制模型的思考长度。<br> 为了尽可能延长模型的思考，他们将模型的思考放在标签内，当结束后，以 final answer 给出答案，同时，当 LLM 即将停止思考时，会强制输出 Wait 来迫使模型继续思考，通过这样的方式，模型会进入反思，并可能会发现自己的错误。<br> 推理时插入的“Wait”，也许会像当初的 Step by Step 一样，成为一个魔法 token。“这或许就是古人‘三思而后行’的哲学吧！”</li>
<li>R1 训练的步骤总结：<br> 1）精心选择若干条（如 8000 条）高质量的数据，<br> 2）通过让 Gemini/DeepSeek V3 补充完善思维链COT之后作为数据集，<br> 3）以开源的大模型（如 Qwen2.5-32B，Llama 3.1）为基座微调出结果(如 R1)。<br> 4）最后，在模型输出时，用（Budget Forcing）预算强制方法强行拉长模型的思考时长和输出 token，结果发现其在特定测试集上进步显著。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/55/">55</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>


<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Weiming Zhuang</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/local-search.js"></script>






  




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>




</body>
</html>
